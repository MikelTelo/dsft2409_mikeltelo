{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelos\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Evaluación y división de datos\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import re  # Para procesar cadenas\n",
    "\n",
    "# Manejo de advertencias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LightGBM (si se usa en otro punto del código)\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "sample_df = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar la columna 'Weight' y convertirla a numérico\n",
    "train_df['Weight'] = train_df['Weight'].str.replace('kg', '').astype(float)\n",
    "test_df['Weight'] = test_df['Weight'].str.replace('kg', '').astype(float)\n",
    "\n",
    "# Procesar la columna 'Ram' y convertirla a numérico\n",
    "train_df['Ram'] = train_df['Ram'].str.replace('GB', '').astype(int)\n",
    "test_df['Ram'] = test_df['Ram'].str.replace('GB', '').astype(int)\n",
    "\n",
    "# Extraer la resolución de pantalla y combinarla en una sola columna 'Resolution'\n",
    "train_df[['res_width', 'res_height']] = train_df['ScreenResolution'].str.extract(r'(\\d{3,4})x(\\d{3,4})').astype(float)\n",
    "test_df[['res_width', 'res_height']] = test_df['ScreenResolution'].str.extract(r'(\\d{3,4})x(\\d{3,4})').astype(float)\n",
    "\n",
    "# Combinar 'res_width' y 'res_height' en una sola columna 'Resolution'\n",
    "train_df['Resolution'] = train_df['res_width'] * train_df['res_height']\n",
    "test_df['Resolution'] = test_df['res_width'] * test_df['res_height']\n",
    "\n",
    "# Eliminar las columnas 'res_width' y 'res_height'\n",
    "train_df = train_df.drop(columns=['res_width', 'res_height'])\n",
    "test_df = test_df.drop(columns=['res_width', 'res_height'])\n",
    "\n",
    "# Procesar la columna 'Memory' para separar SSD, HDD, Flash Storage y Hybrid\n",
    "def parse_memory_details(memory_str):\n",
    "    if isinstance(memory_str, str):\n",
    "        memory_str = memory_str.strip().lower()\n",
    "        ssd, hdd, flash, hybrid = 0, 0, 0, 0\n",
    "        \n",
    "        if 'ssd' in memory_str:\n",
    "            matches = re.findall(r'(\\d+)(tb|gb) ssd', memory_str)\n",
    "            for size, unit in matches:\n",
    "                ssd += int(size) * (1024 if unit == 'tb' else 1)\n",
    "        \n",
    "        if 'hdd' in memory_str:\n",
    "            matches = re.findall(r'(\\d+)(tb|gb) hdd', memory_str)\n",
    "            for size, unit in matches:\n",
    "                hdd += int(size) * (1024 if unit == 'tb' else 1)\n",
    "        \n",
    "        if 'flash storage' in memory_str:\n",
    "            flash = 1\n",
    "        \n",
    "        if 'hybrid' in memory_str:\n",
    "            hybrid = 1\n",
    "        \n",
    "        return ssd, hdd, flash, hybrid\n",
    "    \n",
    "    return 0, 0, 0, 0\n",
    "\n",
    "train_df[['Memory_SSD', 'Memory_HDD', 'Flash_Storage', 'Hybrid']] = train_df['Memory'].apply(\n",
    "    lambda x: pd.Series(parse_memory_details(x))\n",
    ")\n",
    "test_df[['Memory_SSD', 'Memory_HDD', 'Flash_Storage', 'Hybrid']] = test_df['Memory'].apply(\n",
    "    lambda x: pd.Series(parse_memory_details(x))\n",
    ")\n",
    "\n",
    "train_df = train_df.drop(columns=['Memory'])\n",
    "test_df = test_df.drop(columns=['Memory'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar la columna 'Cpu' para extraer detalles\n",
    "def parse_cpu_details(cpu_str):\n",
    "    if isinstance(cpu_str, str):\n",
    "        cpu_str = cpu_str.strip().lower()\n",
    "        cores = re.search(r'(\\d+)\\s*core', cpu_str)\n",
    "        cores = int(cores.group(1)) if cores else 0\n",
    "        frequency = re.search(r'(\\d+\\.?\\d*)\\s*ghz', cpu_str)\n",
    "        frequency = float(frequency.group(1)) if frequency else 0.0\n",
    "        family = 'Intel' if 'intel' in cpu_str else 'AMD' if 'amd' in cpu_str else 'Other'\n",
    "        series = re.search(r'(i\\d|ryzen \\d)', cpu_str)\n",
    "        series = series.group(1) if series else 'Other'\n",
    "        return cores, frequency, family, series\n",
    "    return 0, 0.0, 'Other', 'Other'\n",
    "\n",
    "train_df[['Cpu_Cores', 'Cpu_Frequency', 'Cpu_Family', 'Cpu_Series']] = train_df['Cpu'].apply(\n",
    "    lambda x: pd.Series(parse_cpu_details(x))\n",
    ")\n",
    "test_df[['Cpu_Cores', 'Cpu_Frequency', 'Cpu_Family', 'Cpu_Series']] = test_df['Cpu'].apply(\n",
    "    lambda x: pd.Series(parse_cpu_details(x))\n",
    ")\n",
    "\n",
    "train_df = train_df.drop(columns=['Cpu'])\n",
    "test_df = test_df.drop(columns=['Cpu'])\n",
    "\n",
    "train_df = pd.get_dummies(train_df, columns=['Cpu_Family', 'Cpu_Series'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['Cpu_Family', 'Cpu_Series'], drop_first=True)\n",
    "\n",
    "# Procesar la columna 'Gpu' para extraer detalles\n",
    "def parse_gpu_details(gpu_str):\n",
    "    if isinstance(gpu_str, str):\n",
    "        gpu_str = gpu_str.strip().lower()\n",
    "        brand = 'NVIDIA' if 'nvidia' in gpu_str else 'AMD' if 'amd' in gpu_str else 'Intel' if 'intel' in gpu_str else 'Other'\n",
    "        family = re.search(r'(gtx|rtx|radeon|iris|hd graphics)', gpu_str)\n",
    "        family = family.group(1) if family else 'Other'\n",
    "        model = re.search(r'\\b(\\d+)\\b', gpu_str)\n",
    "        model = int(model.group(1)) if model else 0\n",
    "        return brand, family, model\n",
    "    return 'Other', 'Other', 0\n",
    "\n",
    "train_df[['Gpu_Brand', 'Gpu_Family', 'Gpu_Model']] = train_df['Gpu'].apply(\n",
    "    lambda x: pd.Series(parse_gpu_details(x))\n",
    ")\n",
    "test_df[['Gpu_Brand', 'Gpu_Family', 'Gpu_Model']] = test_df['Gpu'].apply(\n",
    "    lambda x: pd.Series(parse_gpu_details(x))\n",
    ")\n",
    "\n",
    "train_df = pd.get_dummies(train_df, columns=['Gpu_Brand', 'Gpu_Family'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['Gpu_Brand', 'Gpu_Family'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar la columna 'Company' como variable categórica (One-Hot Encoding)\n",
    "train_companies = pd.get_dummies(train_df['Company'], prefix='Company')\n",
    "test_companies = pd.get_dummies(test_df['Company'], prefix='Company')\n",
    "\n",
    "# Asegurar que ambas tablas tengan las mismas columnas\n",
    "common_cols = train_companies.columns.union(test_companies.columns)\n",
    "train_companies = train_companies.reindex(columns=common_cols, fill_value=0)\n",
    "test_companies = test_companies.reindex(columns=common_cols, fill_value=0)\n",
    "\n",
    "# Agregar las columnas procesadas de 'Company' al conjunto principal\n",
    "train_df = pd.concat([train_df, train_companies], axis=1)\n",
    "test_df = pd.concat([test_df, test_companies], axis=1)\n",
    "\n",
    "# Procesar las columnas 'TypeName' y 'OpSys' como variables categóricas (One-Hot Encoding)\n",
    "for col in ['TypeName', 'OpSys']:\n",
    "    train_encoded = pd.get_dummies(train_df[col], prefix=col)\n",
    "    test_encoded = pd.get_dummies(test_df[col], prefix=col)\n",
    "    \n",
    "    # Asegurar columnas comunes\n",
    "    common_cols = train_encoded.columns.union(test_encoded.columns)\n",
    "    train_encoded = train_encoded.reindex(columns=common_cols, fill_value=0)\n",
    "    test_encoded = test_encoded.reindex(columns=common_cols, fill_value=0)\n",
    "    \n",
    "    # Agregar al conjunto principal\n",
    "    train_df = pd.concat([train_df, train_encoded], axis=1)\n",
    "    test_df = pd.concat([test_df, test_encoded], axis=1)\n",
    "    \n",
    "    # Eliminar la columna original\n",
    "    train_df = train_df.drop(columns=[col])\n",
    "    test_df = test_df.drop(columns=[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento completo. Datos listos para modelar.\n"
     ]
    }
   ],
   "source": [
    "# Actualizar las características finales después de asegurar columnas comunes\n",
    "features_to_use_improved = (\n",
    "    ['Inches', 'Ram', 'Weight', 'Resolution', 'Memory_SSD', 'Memory_HDD', 'Flash_Storage', 'Hybrid', 'Cpu_Cores', 'Cpu_Frequency', 'Gpu_Model']\n",
    "    + [col for col in train_df.columns if col.startswith('TypeName_') or col.startswith('OpSys_') or col.startswith('Cpu_Family_') or col.startswith('Cpu_Series_') or col.startswith('Gpu_Brand_') or col.startswith('Gpu_Family_') or col.startswith('Company_')]\n",
    ")\n",
    "\n",
    "# Crear los conjuntos de entrenamiento y prueba\n",
    "X_train_improved = train_df[features_to_use_improved]\n",
    "X_test_improved = test_df[features_to_use_improved]\n",
    "\n",
    "# Manejar columnas faltantes en el conjunto de prueba\n",
    "missing_cols_test = set(X_train_improved.columns) - set(X_test_improved.columns)\n",
    "for col in missing_cols_test:\n",
    "    X_test_improved[col] = 0\n",
    "X_test_improved = X_test_improved[X_train_improved.columns]\n",
    "\n",
    "# Separar la variable objetivo (target)\n",
    "y_train = train_df['Price_euros']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y validación\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_improved, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Aplicar la transformación logarítmica para la variable objetivo\n",
    "y_train_log = np.log1p(y_train_split)\n",
    "y_val_log = np.log1p(y_val_split)\n",
    "\n",
    "print(\"Preprocesamiento completo. Datos listos para modelar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models for Low range...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Low range - MAE: 15.41, RMSE: 20.38, R²: 0.95\n",
      "\n",
      "Training models for Medium range...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Medium range - MAE: 55.00, RMSE: 70.99, R²: 0.98\n",
      "\n",
      "Training models for High range...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "High range - MAE: 188.20, RMSE: 192.54, R²: -12.48\n"
     ]
    }
   ],
   "source": [
    "# Función para asignar rango de precios\n",
    "def assign_price_range(price):\n",
    "    if price <= 500:\n",
    "        return 'Low'\n",
    "    elif 500 < price <= 3000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Asignar rangos de precios en el conjunto de entrenamiento\n",
    "train_df['Price_Range'] = y_train.apply(assign_price_range)\n",
    "val_df = X_val_split.copy()\n",
    "val_df['Price_Range'] = y_val_split.apply(assign_price_range)\n",
    "\n",
    "# Crear diccionario para almacenar modelos y métricas por rango\n",
    "range_results = {}\n",
    "price_ranges = ['Low', 'Medium', 'High']\n",
    "\n",
    "# Hiperparámetros para ajustar por rango\n",
    "hyperparams = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Entrenar modelos por rango de precios\n",
    "for price_range in price_ranges:\n",
    "    print(f\"\\nTraining models for {price_range} range...\")\n",
    "    # Filtrar datos por rango\n",
    "    X_train_range = X_train_improved[train_df['Price_Range'] == price_range]\n",
    "    y_train_range = y_train[train_df['Price_Range'] == price_range]\n",
    "    X_val_range = X_val_split[val_df['Price_Range'] == price_range]\n",
    "    y_val_range = y_val_split[val_df['Price_Range'] == price_range]\n",
    "\n",
    "    # Validación de datos\n",
    "    if X_train_range.empty or X_val_range.empty:\n",
    "        print(f\"Skipping {price_range} range due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    # Ajuste de hiperparámetros con GridSearchCV\n",
    "    model = GradientBoostingRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=hyperparams,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train_range, np.log1p(y_train_range))\n",
    "\n",
    "    # Seleccionar el mejor modelo\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predecir en el conjunto de validación\n",
    "    y_pred_log = best_model.predict(X_val_range)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    # Calcular métricas de evaluación\n",
    "    mae = mean_absolute_error(y_val_range, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_range, y_pred))\n",
    "    r2 = best_model.score(X_val_range, np.log1p(y_val_range))\n",
    "    print(f\"{price_range} range - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    range_results[price_range] = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models for Low range...\n",
      "\n",
      "Testing Random Forest for Low range...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "MAE for Random Forest in Low range: 18.73\n",
      "\n",
      "Testing Gradient Boosting for Low range...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "MAE for Gradient Boosting in Low range: 17.89\n",
      "\n",
      "Testing XGBoost for Low range...\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "MAE for XGBoost in Low range: 2.20\n",
      "\n",
      "Best model for Low range: XGBoost\n",
      "Best MAE: 2.20\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Training models for Medium range...\n",
      "\n",
      "Testing Random Forest for Medium range...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "MAE for Random Forest in Medium range: 111.86\n",
      "\n",
      "Testing Gradient Boosting for Medium range...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "MAE for Gradient Boosting in Medium range: 35.87\n",
      "\n",
      "Testing XGBoost for Medium range...\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "MAE for XGBoost in Medium range: 35.10\n",
      "\n",
      "Best model for Medium range: XGBoost\n",
      "Best MAE: 35.10\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Training models for High range...\n",
      "\n",
      "Testing Random Forest for High range...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "MAE for Random Forest in High range: 160.51\n",
      "\n",
      "Testing Gradient Boosting for High range...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "MAE for Gradient Boosting in High range: 11.35\n",
      "\n",
      "Testing XGBoost for High range...\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "MAE for XGBoost in High range: 9.48\n",
      "\n",
      "Best model for High range: XGBoost\n",
      "Best MAE: 9.48\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 150, 'subsample': 0.8}\n",
      "\n",
      "Low Range Results:\n",
      "  Algorithm: XGBoost\n",
      "  Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200, 'subsample': 0.8}\n",
      "  MAE: 2.20\n",
      "\n",
      "Medium Range Results:\n",
      "  Algorithm: XGBoost\n",
      "  Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "  MAE: 35.10\n",
      "\n",
      "High Range Results:\n",
      "  Algorithm: XGBoost\n",
      "  Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 150, 'subsample': 0.8}\n",
      "  MAE: 9.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Diccionario de modelos a probar\n",
    "models_to_test = {\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Hiperparámetros para cada modelo\n",
    "hyperparams = {\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 150],\n",
    "        'max_depth': [6, 8, 10],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        'n_estimators': [100, 150],\n",
    "        'max_depth': [4, 6],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [150, 200],\n",
    "        'max_depth': [6, 8],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Crear diccionario para almacenar modelos y métricas por rango y algoritmo\n",
    "range_results = {}\n",
    "price_ranges = ['Low', 'Medium', 'High']\n",
    "\n",
    "# Entrenar modelos por rango de precios y algoritmo\n",
    "for price_range in price_ranges:\n",
    "    print(f\"\\nTraining models for {price_range} range...\")\n",
    "    # Filtrar datos por rango\n",
    "    X_train_range = X_train_improved[train_df['Price_Range'] == price_range]\n",
    "    y_train_range = y_train[train_df['Price_Range'] == price_range]\n",
    "    X_val_range = X_val_split[val_df['Price_Range'] == price_range]\n",
    "    y_val_range = y_val_split[val_df['Price_Range'] == price_range]\n",
    "\n",
    "    # Validación de datos\n",
    "    if X_train_range.empty or X_val_range.empty:\n",
    "        print(f\"Skipping {price_range} range due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    best_model = None\n",
    "    best_mae = float('inf')\n",
    "    best_algo = None\n",
    "    best_params = None\n",
    "\n",
    "    # Probar cada algoritmo\n",
    "    for algo_name, model in models_to_test.items():\n",
    "        print(f\"\\nTesting {algo_name} for {price_range} range...\")\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=hyperparams[algo_name],\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train_range, np.log1p(y_train_range))\n",
    "\n",
    "        # Seleccionar el mejor modelo\n",
    "        current_model = grid_search.best_estimator_\n",
    "        y_pred_log = current_model.predict(X_val_range)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        current_mae = mean_absolute_error(y_val_range, y_pred)\n",
    "\n",
    "        print(f\"MAE for {algo_name} in {price_range} range: {current_mae:.2f}\")\n",
    "\n",
    "        # Guardar el mejor modelo\n",
    "        if current_mae < best_mae:\n",
    "            best_model = current_model\n",
    "            best_mae = current_mae\n",
    "            best_algo = algo_name\n",
    "            best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"\\nBest model for {price_range} range: {best_algo}\")\n",
    "    print(f\"Best MAE: {best_mae:.2f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    range_results[price_range] = {\n",
    "        'model': best_model,\n",
    "        'algorithm': best_algo,\n",
    "        'best_params': best_params,\n",
    "        'mae': best_mae\n",
    "    }\n",
    "\n",
    "# Mostrar resultados por rango\n",
    "for price_range, metrics in range_results.items():\n",
    "    print(f\"\\n{price_range} Range Results:\")\n",
    "    print(f\"  Algorithm: {metrics['algorithm']}\")\n",
    "    print(f\"  Best Parameters: {metrics['best_params']}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating initial predictions...\n",
      "Assigning price ranges to the test set...\n",
      "Predicting for Low range...\n",
      "No data found for Low range in test set. Skipping.\n",
      "Predicting for Medium range...\n",
      "Predicting for High range...\n",
      "Archivo de predicciones guardado como laptop_price_predictions_ranges.csv\n",
      "\n",
      "Resumen de Predicciones por Rango:\n",
      "Low range: 0 laptops predicted\n",
      "Medium range: 2 laptops predicted\n",
      "High range: 389 laptops predicted\n"
     ]
    }
   ],
   "source": [
    "# Generar predicciones iniciales si aún no se han generado\n",
    "if 'Price_euros' not in test_df.columns:\n",
    "    print(\"Generating initial predictions...\")\n",
    "    y_pred_test_log = best_model.predict(X_test_improved)  # Asegúrate de que `best_model` esté definido\n",
    "    y_pred_test = np.expm1(y_pred_test_log)  # Invertir la transformación logarítmica\n",
    "    test_df['Price_euros'] = y_pred_test  # Agregar columna de precios predichos al conjunto de prueba\n",
    "else:\n",
    "    print(\"'Price_euros' already exists in test_df. Using existing values.\")\n",
    "\n",
    "# Asignar rangos de precios en el conjunto de prueba\n",
    "print(\"Assigning price ranges to the test set...\")\n",
    "test_df['Price_Range'] = test_df['Price_euros'].apply(assign_price_range)\n",
    "\n",
    "# Crear predicciones por rango\n",
    "y_pred_test_final = []\n",
    "for price_range in price_ranges:\n",
    "    print(f\"Predicting for {price_range} range...\")\n",
    "    # Filtrar datos de prueba para el rango\n",
    "    X_test_range = X_test_improved[test_df['Price_Range'] == price_range]\n",
    "\n",
    "    # Validación de datos\n",
    "    if X_test_range.empty:\n",
    "        print(f\"No data found for {price_range} range in test set. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Usar el modelo correspondiente\n",
    "    model = range_results[price_range]['model']\n",
    "    y_pred_test_log = model.predict(X_test_range)\n",
    "    y_pred_test_range = np.expm1(y_pred_test_log)\n",
    "\n",
    "    # Guardar predicciones\n",
    "    y_pred_test_final.append(pd.Series(y_pred_test_range, index=X_test_range.index))\n",
    "\n",
    "# Combinar resultados\n",
    "if y_pred_test_final:\n",
    "    y_pred_test_combined = pd.concat(y_pred_test_final).sort_index()\n",
    "else:\n",
    "    print(\"Warning: No predictions were made for any range. Check your data or models.\")\n",
    "    y_pred_test_combined = pd.Series(dtype=float)\n",
    "\n",
    "# Guardar predicciones finales en un archivo CSV consolidado\n",
    "if 'id' in test_df.columns:\n",
    "    output_df = test_df[['id']].copy()\n",
    "    output_df['Price_euros'] = y_pred_test_combined\n",
    "else:\n",
    "    output_df = pd.DataFrame({'Price_euros': y_pred_test_combined})\n",
    "\n",
    "output_filename = 'laptop_price_predictions_ranges.csv'\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "print(f\"Archivo de predicciones guardado como {output_filename}\")\n",
    "\n",
    "# Mostrar resumen de errores\n",
    "if not y_pred_test_combined.empty:\n",
    "    print(\"\\nResumen de Predicciones por Rango:\")\n",
    "    for price_range in price_ranges:\n",
    "        count = (test_df['Price_Range'] == price_range).sum()\n",
    "        print(f\"{price_range} range: {count} laptops predicted\")\n",
    "else:\n",
    "    print(\"No predictions to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
