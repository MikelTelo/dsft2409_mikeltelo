{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios ensembling\n",
    "En este ejercicio vas a realizar prediciones sobre un dataset de ciudadanos indios diabéticos. Se trata de un problema de clasificación en el que intentaremos predecir 1 (diabético) 0 (no diabético)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carga las librerias que consideres comunes al notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lee los datos de [esta direccion](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "Los nombres de columnas son:\n",
    "```Python\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0       6   148    72    35     0  33.6  0.627   50      1\n",
       "1       1    85    66    29     0  26.6  0.351   31      0\n",
       "2       8   183    64     0     0  23.3  0.672   32      1\n",
       "3       1    89    66    23    94  28.1  0.167   21      0\n",
       "4       0   137    40    35   168  43.1  2.288   33      1\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
       "763    10   101    76    48   180  32.9  0.171   63      0\n",
       "764     2   122    70    27     0  36.8  0.340   27      0\n",
       "765     5   121    72    23   112  26.2  0.245   30      0\n",
       "766     1   126    60     0     0  30.1  0.349   47      1\n",
       "767     1    93    70    31     0  30.4  0.315   23      0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "df = pd.read_csv(url, names=['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   preg    768 non-null    int64  \n",
      " 1   plas    768 non-null    int64  \n",
      " 2   pres    768 non-null    int64  \n",
      " 3   skin    768 non-null    int64  \n",
      " 4   test    768 non-null    int64  \n",
      " 5   mass    768 non-null    float64\n",
      " 6   pedi    768 non-null    float64\n",
      " 7   age     768 non-null    int64  \n",
      " 8   class   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preg      17\n",
       "plas     136\n",
       "pres      47\n",
       "skin      51\n",
       "test     186\n",
       "mass     248\n",
       "pedi     517\n",
       "age       52\n",
       "class      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bagging\n",
    "Para este apartado tendrás que crear un ensemble utilizando la técnica de bagging ([BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)), mediante la cual combinarás 100 [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Recuerda utilizar también [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) con 10 kfolds.\n",
    "\n",
    "**Para este apartado y siguientes, no hace falta que dividas en train/test**, por hacerlo más sencillo. Simplemente divide tus datos en features y target.\n",
    "\n",
    "Establece una semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características (X) y etiquetas (y)\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos la semilla para reproducibilidad\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_depth=3,random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    estimator = estimator,\n",
    "    n_estimators=100, # Cantidad de árboles\n",
    "    max_samples=100, # Muestras utilizadas en boostrapping\n",
    "    bootstrap=True, # Usamos boostrapping\n",
    "    # max_features = 3 # Features que utiliza en el boostrapping. Cuanto más bajo, mejor generalizará y menos overfitting\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "bag_clf.fit(X, y)\n",
    "y_pred = bag_clf.predict(X)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio del modelo Bagging: 0.771\n"
     ]
    }
   ],
   "source": [
    "# Validación cruzada\n",
    "kfold = 10\n",
    "scores = cross_val_score(bag_clf, X, y, cv = kfold, scoring ='accuracy')\n",
    "\n",
    "# Mostrar resultados\n",
    "bag_clf_mean_accuracy = np.mean(scores)\n",
    "print(f\"Precisión promedio del modelo Bagging: {bag_clf_mean_accuracy:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest\n",
    "En este caso entrena un [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) con 100 árboles y un `max_features` de 3. También con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7643229166666666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100,\n",
    "                                 max_leaf_nodes=3,\n",
    "                                 random_state=42)\n",
    "rnd_clf.fit(X, y)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X)\n",
    "# np.sum(y_test == y_pred_rf) / len(y_test) \n",
    "accuracy_score(y, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio del modelo Bagging: 0.758\n"
     ]
    }
   ],
   "source": [
    "# Validación cruzada\n",
    "scores = cross_val_score(rnd_clf, X, y, cv = kfold, scoring ='accuracy')\n",
    "\n",
    "# Mostrar resultados\n",
    "rnd_clf_mean_accuracy = np.mean(scores)\n",
    "print(f\"Precisión promedio del modelo Bagging: {rnd_clf_mean_accuracy:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GradientBoosting\n",
    "Implementa un [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) con 100 estimadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947916666666666"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbct = GradientBoostingClassifier(max_depth=2,\n",
    "                                 n_estimators=100,\n",
    "                                 learning_rate=1.0,\n",
    "                                 random_state=42)\n",
    "gbct.fit(X, y)\n",
    "\n",
    "\n",
    "y_pred_gbct = gbct.predict(X)\n",
    "accuracy_score(y, y_pred_gbct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada para Gradient Boosting\n",
    "gb_scores = cross_val_score(gbct, X, y, cv=kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio del modelo Gradient Boosting: 0.705\n"
     ]
    }
   ],
   "source": [
    "# Promedio de precisión\n",
    "gb_mean_accuracy = np.mean(gb_scores)\n",
    "print(f\"Precisión promedio del modelo Gradient Boosting: {gb_mean_accuracy:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. XGBoost\n",
    "Para este apartado utiliza un [XGBoostClassifier](https://docs.getml.com/latest/api/getml.predictors.XGBoostClassifier.html) con 100 estimadores. XGBoost no forma parte de la suite de modelos de sklearn, por lo que tendrás que instalarlo con pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_clas = xgboost.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "xgb_clas.fit(X, y)\n",
    "y_pred = xgb_clas.predict(X)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada para Gradient Boosting\n",
    "xgb_scores = cross_val_score(xgb_clas, X, y, cv=kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio del modelo Gradient Boosting: 0.743\n"
     ]
    }
   ],
   "source": [
    "# Promedio de precisión\n",
    "xgb_mean_accuracy = np.mean(xgb_scores)\n",
    "print(f\"Precisión promedio del modelo Gradient Boosting: {xgb_mean_accuracy:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Primeros resultados\n",
    "Crea un dataframe con los resultados y sus algoritmos, ordenándolos de mayor a menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Algoritmo  Precisión Promedio\n",
      "0            Bagging            0.770848\n",
      "1      Random Forest            0.757826\n",
      "3            XGBoost            0.743472\n",
      "2  Gradient Boosting            0.704511\n"
     ]
    }
   ],
   "source": [
    "# Resultados de los modelos\n",
    "resultados = {\n",
    "    \"Algoritmo\": [\"Bagging\", \"Random Forest\", \"Gradient Boosting\", \"XGBoost\"],\n",
    "    \"Precisión Promedio\": [bag_clf_mean_accuracy, rnd_clf_mean_accuracy, gb_mean_accuracy, xgb_mean_accuracy]  # Incluye los valores calculados previamente\n",
    "}\n",
    "\n",
    "# Crear un DataFrame\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "\n",
    "# Ordenar los resultados de mayor a menor\n",
    "resultados_df = resultados_df.sort_values(by=\"Precisión Promedio\", ascending=False)\n",
    "\n",
    "# Mostrar el DataFrame ordenado\n",
    "print(resultados_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Hiperparametrización\n",
    "Vuelve a entrenar los modelos de nuevo, pero esta vez dividiendo el conjunto de datos en train/test y utilizando un gridsearch para encontrar los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Mejores hiperparámetros: {'estimator__max_depth': 3, 'max_samples': 20, 'n_estimators': 200}\n",
      "Mejor puntuación (accuracy): 0.7622151139544182\n",
      "Accuracy Bagging en test: 0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier(max_depth=3,random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    estimator = estimator,\n",
    "    n_estimators=100, # Cantidad de árboles\n",
    "    max_samples=100, # Muestras utilizadas en boostrapping\n",
    "    bootstrap=True, # Usamos boostrapping\n",
    "    # max_features = 3 # Features que utiliza en el boostrapping. Cuanto más bajo, mejor generalizará y menos overfitting\n",
    "    random_state=42)\n",
    "\n",
    "# Define el rango de hiperparámetros para el Grid Search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"max_samples\": [2, 5, 10, 20],\n",
    "    \"estimator__max_depth\": [3, 5, 7, None]    # Profundidad máxima del árbol\n",
    "}\n",
    "\n",
    "# Configura el Grid Search con Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=bag_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",  # Métrica a optimizar\n",
    "    cv=5,  # Número de divisiones de Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajusta el modelo con los datos\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime los mejores hiperparámetros y el mejor puntaje\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "# EXtraer el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realiza predicciones con el modelo optimizado\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de rendimiento (opcional)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Bagging en test:\", accuracy_bagging)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Mejores hiperparámetros: {'max_leaf_nodes': 20, 'n_estimators': 200}\n",
      "Mejor puntuación (accuracy): 0.7785419165667067\n",
      "Accuracy Bagging en test: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define el rango de hiperparámetros para el Grid Search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20],\n",
    "}\n",
    "\n",
    "# Configura el Grid Search con Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rnd_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",  # Métrica a optimizar\n",
    "    cv=5,  # Número de divisiones de Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajusta el modelo con los datos\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime los mejores hiperparámetros y el mejor puntaje\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "# EXtraer el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realiza predicciones con el modelo optimizado\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de rendimiento (opcional)\n",
    "accuracy_Random_Forest = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Bagging en test:\", accuracy_Random_Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Mejores hiperparámetros: {'max_depth': 50, 'n_estimators': 2}\n",
      "Mejor puntuación (accuracy): 0.7214980674396907\n",
      "Accuracy  Gradient Boosting en test: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "gbct = GradientBoostingClassifier(learning_rate=1.0,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Define el rango de hiperparámetros para el Grid Search\n",
    "param_grid = {\n",
    "    \"max_depth\": [50, 100, 150, 200],\n",
    "    \"n_estimators\": [2, 5, 10, 20],\n",
    "}\n",
    "\n",
    "# Configura el Grid Search con Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbct,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",  # Métrica a optimizar\n",
    "    cv=5,  # Número de divisiones de Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajusta el modelo con los datos\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime los mejores hiperparámetros y el mejor puntaje\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "# EXtraer el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realiza predicciones con el modelo optimizado\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de rendimiento (opcional)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_Gradient_Boosting = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy  Gradient Boosting en test:\", accuracy_Gradient_Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Mejores hiperparámetros: {'n_estimators': 5}\n",
      "Mejor puntuación (accuracy): 0.7524590163934426\n",
      "Accuracy XGBoost en test: 0.7337662337662337\n"
     ]
    }
   ],
   "source": [
    "xgb_clas = xgboost.XGBClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "# Define el rango de hiperparámetros para el Grid Search\n",
    "param_grid = {\n",
    "    #\"max_depth\": [50, 100, 150, 200],\n",
    "    \"n_estimators\": [2, 5, 10, 20],\n",
    "}\n",
    "\n",
    "# Configura el Grid Search con Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clas,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",  # Métrica a optimizar\n",
    "    cv=5,  # Número de divisiones de Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajusta el modelo con los datos\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime los mejores hiperparámetros y el mejor puntaje\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "# EXtraer el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realiza predicciones con el modelo optimizado\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de rendimiento (opcional)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_XGBoost  = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy XGBoost en test:\", accuracy_XGBoost )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conclusiones finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Algoritmo  Precisión Promedio\n",
      "0            Bagging            0.770848\n",
      "1      Random Forest            0.757826\n",
      "3            XGBoost            0.743472\n",
      "2  Gradient Boosting            0.704511\n"
     ]
    }
   ],
   "source": [
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Algoritmo  Precisión Promedio\n",
      "0            Bagging            0.766234\n",
      "1      Random Forest            0.746753\n",
      "2  Gradient Boosting            0.746753\n",
      "3            XGBoost            0.733766\n"
     ]
    }
   ],
   "source": [
    "# Resultados de los modelos\n",
    "resultados2 = {\n",
    "    \"Algoritmo\": [\"Bagging\", \"Random Forest\", \"Gradient Boosting\", \"XGBoost\"],\n",
    "    \"Precisión Promedio\": [accuracy_bagging, accuracy_Random_Forest, accuracy_Gradient_Boosting, accuracy_XGBoost]\n",
    "}\n",
    "\n",
    "# Crear un DataFrame\n",
    "resultados_df2 = pd.DataFrame(resultados2)\n",
    "\n",
    "# Ordenar los resultados de mayor a menor precisión promedio\n",
    "resultados_df2 = resultados_df2.sort_values(by=\"Precisión Promedio\", ascending=False)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Mejores hiperparámetros: {'estimator__max_depth': 3, 'max_samples': 20, 'n_estimators': 200}\n",
      "Mejor puntuación (accuracy): 0.7622151139544182\n",
      "Accuracy Bagging en test: 0.7662337662337663\n",
      "     preg  plas  pres  skin  test  mass   pedi  age  class  Predicción  \\\n",
      "0       6   148    72    35     0  33.6  0.627   50      1           1   \n",
      "1       1    85    66    29     0  26.6  0.351   31      0           0   \n",
      "2       8   183    64     0     0  23.3  0.672   32      1           1   \n",
      "3       1    89    66    23    94  28.1  0.167   21      0           0   \n",
      "4       0   137    40    35   168  43.1  2.288   33      1           1   \n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...         ...   \n",
      "763    10   101    76    48   180  32.9  0.171   63      0           0   \n",
      "764     2   122    70    27     0  36.8  0.340   27      0           0   \n",
      "765     5   121    72    23   112  26.2  0.245   30      0           0   \n",
      "766     1   126    60     0     0  30.1  0.349   47      1           0   \n",
      "767     1    93    70    31     0  30.4  0.315   23      0           0   \n",
      "\n",
      "    Es Diabético  \n",
      "0             Sí  \n",
      "1             No  \n",
      "2             Sí  \n",
      "3             No  \n",
      "4             Sí  \n",
      "..           ...  \n",
      "763           No  \n",
      "764           No  \n",
      "765           No  \n",
      "766           No  \n",
      "767           No  \n",
      "\n",
      "[768 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier(max_depth=3,random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    estimator = estimator,\n",
    "    bootstrap=True, # Usamos boostrapping\n",
    "    # max_features = 3 # Features que utiliza en el boostrapping. Cuanto más bajo, mejor generalizará y menos overfitting\n",
    "    random_state=42)\n",
    "\n",
    "# Define el rango de hiperparámetros para el Grid Search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"max_samples\": [2, 5, 10, 20],\n",
    "    \"estimator__max_depth\": [3, 5, 7, None]    # Profundidad máxima del árbol\n",
    "}\n",
    "\n",
    "# Configura el Grid Search con Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=bag_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",  # Métrica a optimizar\n",
    "    cv=5,  # Número de divisiones de Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajusta el modelo con los datos\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime los mejores hiperparámetros y el mejor puntaje\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "# EXtraer el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realiza predicciones con el modelo optimizado\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de rendimiento (opcional)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Bagging en test:\", accuracy_bagging)\n",
    "\n",
    "# Usar el mejor modelo para predecir en todo el DataFrame\n",
    "df['Predicción'] = best_model.predict(X)\n",
    "\n",
    "# Interpretar las clases (Opcional)\n",
    "df['Es Diabético'] = df['Predicción'].apply(lambda x: 'Sí' if x == 1 else 'No')\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones\n",
    "print(df[['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class', 'Predicción', 'Es Diabético']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
