{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 9000/10000 completado. Epsilon: 0.010\n",
      "Entrenamiento completado!\n",
      "\n",
      "Ejecutando el agente entrenado...\n",
      "Episodio 1: Recompensa total = -192.0\n",
      "Episodio 2: Recompensa total = -192.0\n",
      "Episodio 3: Recompensa total = -211.0\n",
      "Episodio 4: Recompensa total = -200.0\n",
      "Episodio 5: Recompensa total = -207.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hegoi\\TheBridge\\2409_dsft_thebridge_core\\.venv\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py:171: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"MountainCar-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Configuración del entorno\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=None)  # Cambiar render_mode=\"human\" si quieres una ventana gráfica\n",
    "\n",
    "# Discretización del espacio de estados\n",
    "state_bins = [20, 20]  # Número de divisiones en cada dimensión del espacio de estados\n",
    "position_space = np.linspace(-1.2, 0.6, state_bins[0])\n",
    "velocity_space = np.linspace(-0.07, 0.07, state_bins[1])\n",
    "\n",
    "def discretize_state(state):\n",
    "    \"\"\"Convierte un estado continuo en un estado discreto.\"\"\"\n",
    "    position, velocity = state\n",
    "    position_idx = np.digitize(position, position_space) - 1\n",
    "    velocity_idx = np.digitize(velocity, velocity_space) - 1\n",
    "    return (position_idx, velocity_idx)\n",
    "\n",
    "# Inicialización de la tabla Q\n",
    "q_table = np.zeros((*state_bins, env.action_space.n))  # (20, 20, 3)\n",
    "\n",
    "# Hiperparámetros\n",
    "alpha = 0.1        # Tasa de aprendizaje\n",
    "gamma = 0.99       # Factor de descuento\n",
    "epsilon = 1.0      # Tasa de exploración inicial\n",
    "epsilon_decay = 0.999  # Decaimiento de epsilon\n",
    "min_epsilon = 0.01  # Valor mínimo de epsilon\n",
    "episodes = 10000    # Número de episodios de entrenamiento\n",
    "\n",
    "# Entrenamiento del agente\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    state = discretize_state(state)  # Convertir estado continuo en discreto\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Selección de acción: explorar o explotar\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explorar\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Explotar\n",
    "\n",
    "        # Ejecutar acción\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = discretize_state(next_state)  # Discretizar el nuevo estado\n",
    "\n",
    "        # Recompensa ajustada para motivar al agente a avanzar\n",
    "        reward = next_state[0] / len(position_space)\n",
    "\n",
    "        # Actualización de la tabla Q\n",
    "        old_value = q_table[state + (action,)]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        q_table[state + (action,)] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        # Actualizar estado y recompensa acumulada\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Reducir epsilon gradualmente\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Mostrar progreso\n",
    "    if episode % 1000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episodio {episode}/{episodes} completado. Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "print(\"Entrenamiento completado!\")\n",
    "\n",
    "# Evaluación del agente entrenado\n",
    "test_episodes = 5\n",
    "print(\"\\nEjecutando el agente entrenado...\")\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()  # Mostrar el entorno en una ventana gráfica\n",
    "        action = np.argmax(q_table[state])  # Elegir la mejor acción\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episodio {episode + 1}: Recompensa total = {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
