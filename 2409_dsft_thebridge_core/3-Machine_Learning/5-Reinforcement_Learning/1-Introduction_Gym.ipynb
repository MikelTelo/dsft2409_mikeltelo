{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considera el escenario de enseñarle nuevos trucos a un perro. El perro no entiende nuestro lenguaje, así que no podemos decirle qué hacer. En cambio, seguimos una estrategia diferente. Emulamos una situación (o una señal), y el perro intenta responder de muchas maneras diferentes. Si la respuesta del perro es la deseada, lo recompensamos con golosinas. Ahora, adivina qué, la próxima vez que el perro se enfrenta a la misma situación, ejecuta una acción similar con aún más entusiasmo esperando más comida. Esto es como aprender \"qué hacer\" a partir de experiencias positivas. De manera similar, los perros tienden a aprender qué no hacer cuando se enfrentan a experiencias negativas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendiendo cómo funciona el Aprendizaje por Refuerzo\n",
    "\n",
    "En un sentido más amplio, así es como funciona el Aprendizaje por Refuerzo:\n",
    "\n",
    "* Tu perro es un \"agente\" que está expuesto al entorno. El entorno podría ser tu casa, contigo.\n",
    "* Las situaciones que encuentran son análogas a un estado. Un ejemplo de un estado podría ser tu perro de pie y tú usando una palabra específica con cierto tono en tu sala de estar.\n",
    "* Nuestros agentes reaccionan realizando una acción para pasar de un \"estado\" a otro \"estado\"; por ejemplo, tu perro pasa de estar de pie a sentarse.\n",
    "* Después de la transición, pueden recibir una recompensa o una penalización a cambio. ¡Les das una golosina! O un \"No\" como penalización.\n",
    "* La política es la estrategia de elegir una acción dada un estado en espera de mejores resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Aprendizaje por Refuerzo se encuentra entre el espectro del Aprendizaje Supervisado y el No Supervisado, y hay algunas cosas importantes que tener en cuenta:\n",
    "\n",
    "#### No siempre es beneficioso ser codicioso\n",
    "\n",
    "**Ser codicioso no siempre funciona:**\n",
    "\n",
    "* Hay cosas que son fáciles de hacer para obtener una gratificación instantánea, y hay cosas que proporcionan recompensas a largo plazo. El objetivo no es ser codicioso buscando las recompensas inmediatas, sino optimizar las recompensas máximas durante todo el entrenamiento.\n",
    "\n",
    "#### La secuencia importa en el Aprendizaje por Refuerzo\n",
    "\n",
    "**La secuencia importa en el Aprendizaje por Refuerzo:**\n",
    "\n",
    "* La recompensa del agente no solo depende del estado actual, sino de toda la historia de estados. A diferencia del aprendizaje supervisado y no supervisado, el tiempo es importante aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Reinforcement-Learning-Animation.gif\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cierto sentido, el Aprendizaje por Refuerzo es la ciencia de tomar decisiones óptimas utilizando experiencias. Desglosándolo, el proceso de Aprendizaje por Refuerzo involucra estos pasos simples:\n",
    "\n",
    "1. Observación del entorno.\n",
    "2. Decidir cómo actuar utilizando alguna estrategia.\n",
    "3. Actuar en consecuencia.\n",
    "4. Recibir una recompensa o penalización.\n",
    "5. Aprender de las experiencias y refinar nuestra estrategia.\n",
    "6. Iterar hasta encontrar una estrategia óptima.\n",
    "\n",
    "Ahora vamos a entender el Aprendizaje por Refuerzo desarrollando un agente para aprender a jugar un juego automáticamente por sí mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comenzando con Gym\n",
    "\n",
    "Gym es un conjunto de herramientas para desarrollar y comparar algoritmos de aprendizaje por refuerzo. No hace suposiciones sobre la estructura de tu agente y es compatible con cualquier biblioteca de cálculo numérico, como TensorFlow o Theano.\n",
    "\n",
    "La biblioteca Gym es una colección de problemas de prueba, o entornos, que puedes usar para desarrollar tus algoritmos de aprendizaje por refuerzo. Estos entornos tienen una interfaz compartida, lo que te permite escribir algoritmos generales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym\n",
    "# pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entornos\n",
    "\n",
    "Aquí tienes un ejemplo mínimo para comenzar a ejecutar algo. Esto ejecutará una instancia del entorno CartPole-v0 durante 1000 pasos de tiempo, representando el entorno en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consiste en equilibrar un poste (péndulo) montado en la parte superior de un carrito móvil:\n",
    " * El **objetivo** es mantener el poste en posición vertical mientras el carrito se mueve hacia adelante y hacia atrás en una pista. \n",
    " * El **agente** (o jugador) tiene dos acciones disponibles en cada paso de tiempo: empujar el carrito hacia la izquierda o hacia la derecha. \n",
    " * El **desafío** radica en tomar decisiones adecuadas para evitar que el poste caiga mientras se mueve el carrito. \n",
    " \n",
    " Este problema es un ejemplo comúnmente utilizado para probar algoritmos de aprendizaje por refuerzo debido a su simplicidad y naturaleza desafiante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/api/env/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import time\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for i in range(300):\n",
    "    env.render()\n",
    "    # time.sleep(0.1)\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente, terminaremos la simulación antes de que el carrito-péndulo tenga permitido salir de la pantalla. Más sobre eso después. Por ahora, por favor ignora la advertencia sobre llamar a `step()` aunque este entorno ya haya devuelto `done = True`.\n",
    "\n",
    "Si deseas ver otros entornos en acción, intenta reemplazar `CartPole-v0` arriba con algo como `MountainCar-v0`, `MsPacman-v0` (requiere la dependencia de Atari), o `Hopper-v1` (requiere las dependencias de MuJoCo). Todos los entornos descienden de la clase base `Env`.\n",
    "\n",
    "Ten en cuenta que si te faltan dependencias, deberías recibir un mensaje de error útil que te diga qué te falta. (Avísanos si alguna dependencia te causa problemas sin una instrucción clara para solucionarlo). Instalar una dependencia faltante generalmente es bastante simple. También necesitarás una licencia de MuJoCo para `Hopper-v1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones\n",
    "\n",
    "Si queremos hacer algo mejor que tomar acciones aleatorias en cada paso, probablemente sería bueno saber realmente qué están haciendo nuestras acciones en el entorno.\n",
    "\n",
    "La función `step` del entorno devuelve exactamente lo que necesitamos. De hecho, `step` devuelve cuatro valores. Estos son:\n",
    "\n",
    "* `observation` (object): un objeto específico del entorno que representa tu observación del entorno. Por ejemplo, datos de píxeles de una cámara, ángulos de articulación y velocidades de articulación de un robot, o el estado del tablero en un juego de mesa.\n",
    "* `reward` (float): cantidad de recompensa lograda por la acción anterior. La escala varía entre entornos, pero el objetivo es siempre aumentar tu recompensa total.\n",
    "* `done` (bool): si es hora de restablecer el entorno nuevamente. La mayoría (pero no todos) de las tareas están divididas en episodios bien definidos, y `done` siendo True indica que el episodio ha terminado. (Por ejemplo, tal vez el poste se inclinó demasiado, o perdiste tu última vida).\n",
    "* `truncated` (bool): True si el episodio se trunca debido a un límite de tiempo o una razón que no está definida.\n",
    "* `info` (dict): información de diagnóstico útil para la depuración. A veces puede ser útil para el aprendizaje (por ejemplo, podría contener las probabilidades crudas detrás del último cambio de estado del entorno). Sin embargo, no se permite usar esto para el aprendizaje en las evaluaciones oficiales de tu agente.\n",
    "\n",
    "Esto es simplemente una implementación del clásico \"bucle agente-entorno\". En cada paso de tiempo, el agente elige una acción, y el entorno devuelve una observación y una recompensa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Acciones\n",
    "\n",
    "Hay 3 acciones discretas determinísticas:\n",
    "\n",
    "| Num | Observación          | Valor | Unidad       |\n",
    "|-----|----------------------|-------|--------------|\n",
    "| 0   | Acelerar a la izquierda | Inf   | posición (m) |\n",
    "| 1   | No acelerar              | Inf   | posición (m) |\n",
    "| 2   | Acelerar a la derecha    | Inf   | posición (m) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('MountainCar-v0', render_mode='human')\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Observación\n",
    "\n",
    "La observación es un ndarray con forma (2,), donde los elementos corresponden a lo siguiente:\n",
    "\n",
    "| Num | Observación                    | Mínimo | Máximo | Unidad        |\n",
    "|-----|--------------------------------|--------|--------|---------------|\n",
    "| 0   | posición del coche a lo largo del eje x | -Inf   | Inf    | posición (m) |\n",
    "| 1   | velocidad del coche             | -Inf   | Inf    | velocidad (m) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intento 0\n",
      "Accion 0\n",
      "Observación (array([-0.5468764,  0.       ], dtype=float32), {})\n",
      "Accion 10\n",
      "Observación [-0.48652607  0.01034372]\n",
      "Accion 20\n",
      "Observación [-0.35796544  0.01352569]\n",
      "Accion 30\n",
      "Observación [-0.24615523  0.00823464]\n",
      "Accion 40\n",
      "Observación [-0.21491736 -0.0013152 ]\n",
      "Accion 50\n",
      "Observación [-0.27965376 -0.01024393]\n",
      "Accion 60\n",
      "Observación [-0.40715685 -0.01359465]\n",
      "Accion 70\n",
      "Observación [-0.51998866 -0.00810069]\n",
      "Accion 80\n",
      "Observación [-0.5413371   0.00305324]\n",
      "Accion 90\n",
      "Observación [-0.4562258  0.0120595]\n",
      "Intento 1\n",
      "Accion 0\n",
      "Observación (array([-0.567218,  0.      ], dtype=float32), {})\n",
      "Accion 10\n",
      "Observación [-0.49905214  0.01168456]\n",
      "Accion 20\n",
      "Observación [-0.3539      0.01526095]\n",
      "Accion 30\n",
      "Observación [-0.22761634  0.00934483]\n",
      "Accion 40\n",
      "Observación [-0.1905489  -0.00118367]\n",
      "Accion 50\n",
      "Observación [-0.2598734  -0.01119465]\n",
      "Accion 60\n",
      "Observación [-0.4020463  -0.01538466]\n",
      "Accion 70\n",
      "Observación [-0.5325829  -0.00965503]\n",
      "Accion 80\n",
      "Observación [-0.5627829   0.00282263]\n",
      "Accion 90\n",
      "Observación [-0.4714548   0.01331532]\n",
      "Intento 2\n",
      "Accion 0\n",
      "Observación (array([-0.45300123,  0.        ], dtype=float32), {})\n",
      "Accion 10\n",
      "Observación [-0.42857876  0.00419307]\n",
      "Accion 20\n",
      "Observación [-0.37608632  0.00555983]\n",
      "Accion 30\n",
      "Observación [-0.33005023  0.00335255]\n",
      "Accion 40\n",
      "Observación [-0.3191698  -0.00088667]\n",
      "Accion 50\n",
      "Observación [-0.3499712  -0.00459875]\n",
      "Accion 60\n",
      "Observación [-0.40371522 -0.00546225]\n",
      "Accion 70\n",
      "Observación [-0.4459522  -0.00273534]\n",
      "Accion 80\n",
      "Observación [-0.4483522   0.00185387]\n",
      "Accion 90\n",
      "Observación [-0.40927833  0.00517694]\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "for i_episode in range(3):\n",
    "    # Reset del entorno\n",
    "    print(\"Intento\", i_episode)\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        if t % 10 == 0:\n",
    "            print(\"Accion\", t)\n",
    "            print(\"Observación\", observation)\n",
    "        # Visualizar ejercicio\n",
    "        env.render()\n",
    "        # Print del estado\n",
    "        # print(observation)\n",
    "        # Guarda en la variable action una de las acciones posibles elegida al azar\n",
    "        action = env.action_space.sample()\n",
    "        # Ejecuta esa acción, lo que nos devuelve, un nuevo estado, una recompensa, booleana que nos indica que ha alcanzado su objetivo e info debug\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
