{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Cab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulación de un Taxi Autónomo\n",
    "\n",
    "Diseñemos una simulación de un taxi autónomo. El objetivo principal es demostrar, en un entorno simplificado, cómo se pueden utilizar técnicas de aprendizaje por refuerzo para desarrollar un enfoque eficiente y seguro para abordar este problema.\n",
    "\n",
    "El trabajo del Smartcab es recoger al pasajero en una ubicación y dejarlo en otra. Aquí hay algunas cosas que nos encantaría que nuestro Smartcab se encargara:\n",
    "\n",
    "- Dejar al pasajero en la ubicación correcta.\n",
    "- Ahorrar tiempo al pasajero tomando el tiempo mínimo posible para dejarlo.\n",
    "- Cuidar la seguridad del pasajero y las normas de tráfico.\n",
    "\n",
    "Hay diferentes aspectos que deben considerarse aquí al modelar una solución de aprendizaje por refuerzo para este problema: recompensas, estados y acciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompensa y Penalización\n",
    "\n",
    "Dado que el agente (el conductor imaginario) está motivado por las recompensas y aprenderá a controlar el taxi mediante experiencias de prueba en el entorno, necesitamos decidir las recompensas y/o penalizaciones y su magnitud en consecuencia. Aquí hay algunos puntos a considerar:\n",
    "\n",
    "* El agente debería recibir una recompensa positiva alta por una entrega exitosa porque este comportamiento es altamente deseado.\n",
    "* El agente debería ser penalizado si intenta dejar al pasajero en ubicaciones incorrectas.\n",
    "* El agente debería recibir una ligera penalización negativa por no llegar al destino después de cada paso de tiempo. \"Ligera\" negativa porque preferiríamos que nuestro agente llegue tarde en lugar de hacer movimientos incorrectos tratando de llegar al destino lo más rápido posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space\n",
    "\n",
    "En Aprendizaje por Refuerzo, el agente encuentra un estado y luego toma una acción de acuerdo con el estado en el que se encuentra.\n",
    "\n",
    "El Espacio de Estados es el conjunto de todas las situaciones posibles en las que nuestro taxi podría encontrarse. El estado debe contener información útil que el agente necesita para tomar la acción correcta.\n",
    "\n",
    "Digamos que tenemos un área de entrenamiento para nuestro Smartcab donde lo estamos enseñando a transportar personas en un estacionamiento a cuatro ubicaciones diferentes (R, G, Y, B):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Reinforcement_Learning_Taxi_Env.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Estados\n",
    "\n",
    "Supongamos que Smartcab es el único vehículo en este estacionamiento. Podemos dividir el estacionamiento en una cuadrícula de 5x5, lo que nos da 25 ubicaciones de taxi posibles. Estas 25 ubicaciones son una parte de nuestro espacio de estados. Observa que el estado de ubicación actual de nuestro taxi es la coordenada (3, 1).\n",
    "\n",
    "También notarás que hay cuatro (4) ubicaciones donde podemos recoger y dejar a un pasajero: R, G, Y, B o [(0,0), (0,4), (4,0), (4,3)] en coordenadas (fila, columna). Nuestro pasajero ilustrado está en la ubicación Y y desea ir a la ubicación R.\n",
    "\n",
    "Cuando también tenemos en cuenta un (1) estado adicional del pasajero de estar dentro del taxi, podemos tomar todas las combinaciones de ubicaciones de pasajeros y ubicaciones de destino para llegar a un número total de estados para nuestro entorno de taxi; hay cuatro (4) destinos y cinco (4 + 1) ubicaciones de pasajeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, our taxi environment has a total of possible states:\n",
    "5*5*4*(4+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones en el Entorno del Taxi\n",
    "\n",
    "El agente encuentra uno de los 500 estados y toma una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger/dejar a un pasajero.\n",
    "\n",
    "En otras palabras, tenemos seis acciones posibles:\n",
    "1. sur\n",
    "2. norte\n",
    "3. este\n",
    "4. oeste\n",
    "5. recoger\n",
    "6. dejar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Acciones\n",
    "\n",
    "Este es el espacio de acciones: el conjunto de todas las acciones que nuestro agente puede tomar en un estado dado.\n",
    "\n",
    "Observarás en la ilustración anterior que el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes. En el código del entorno, simplemente proporcionaremos una penalización de -1 por cada colisión con una pared y el taxi no se moverá en absoluto. Esto simplemente acumulará penalizaciones, haciendo que el taxi considere ir alrededor de la pared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env\n",
    "env.reset()\n",
    "print(env.render())\n",
    "# time.sleep(10)\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz del Entorno Gym\n",
    "\n",
    "La interfaz central de Gym es `env`, que es la interfaz de entorno unificada. Los siguientes son los métodos de `env` que serían bastante útiles para nosotros:\n",
    "\n",
    "* `env.reset`: Reinicia el entorno y devuelve un estado inicial aleatorio.\n",
    "* `env.step(action)`: Avanza el entorno en un paso de tiempo. Devuelve\n",
    "    - `observation`: Observaciones del entorno\n",
    "    - `reward`: Si tu acción fue beneficiosa o no\n",
    "    - `done`: Indica si hemos recogido y dejado con éxito a un pasajero, también llamado un episodio\n",
    "    - `truncated`: Si el episodio se trunca debido a un límite de tiempo o una razón que no está definida.\n",
    "    - `info`: Información adicional como rendimiento y latencia con fines de depuración\n",
    "* `env.render`: Renderiza un fotograma del entorno (útil para visualizar el entorno)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hay 4 ubicaciones (etiquetadas con diferentes letras), y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Recibimos +20 puntos por una entrega exitosa y perdemos 1 punto por cada paso de tiempo que toma. También hay una penalización de 10 puntos por acciones de recogida y entrega ilegales.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "print(env.render())\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El cuadrado lleno representa el taxi, que es amarillo sin pasajero y verde con pasajero.\n",
    "* El símbolo \"|\" representa una pared que el taxi no puede cruzar.\n",
    "* R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida del pasajero, y la letra morada es el destino actual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según lo verificado por las impresiones, tenemos un Espacio de Acciones de tamaño 6 y un Espacio de Estados de tamaño 500. Como verás, nuestro algoritmo de aprendizaje por refuerzo no necesitará más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de manera única asignando un número único a cada estado posible, y el aprendizaje por refuerzo aprende a elegir un número de acción de 0 a 5 donde:\n",
    "\n",
    "0 = sur\n",
    "1 = norte\n",
    "2 = este\n",
    "3 = oeste\n",
    "4 = recoger\n",
    "5 = dejar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que los 500 estados corresponden a una codificación de la ubicación del taxi, la ubicación del pasajero y la ubicación de destino.\n",
    "\n",
    "El Aprendizaje por Refuerzo aprenderá un mapeo de estados a la acción óptima a realizar en ese estado mediante la exploración, es decir, el agente explora el entorno y toma acciones basadas en las recompensas definidas en el entorno.\n",
    "\n",
    "La acción óptima para cada estado es la acción que tiene la recompensa acumulativa a largo plazo más alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De hecho, podemos tomar nuestra ilustración anterior, codificar su estado y dárselo al entorno para que lo renderice en Gym. Recuerda que tenemos el taxi en la fila 3, columna 1, nuestro pasajero está en la ubicación 2 y nuestro destino está en la ubicación 0. Usando el método de codificación de estado Taxi-v3, podemos hacer lo siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 324\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 1, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.unwrapped.s = state\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 166\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crea un state donde el taxi se encuentra en la fila 2, columna 3, y el pasajero está en la G con Destino Y\n",
    "state = env.encode(1, 3, 1, 2) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.unwrapped.s = state\n",
    "print(env.render())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos utilizando las coordenadas de nuestra ilustración para generar un número correspondiente a un estado entre 0 y 499, lo cual resulta ser 324 para el estado de nuestra ilustración.\n",
    "\n",
    "Luego podemos establecer manualmente el estado del entorno utilizando env.unwrapped.s con ese número codificado. Puedes experimentar con los números y verás que el taxi, el pasajero y el destino se mueven.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reward Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se crea el entorno Taxi, también se crea una tabla de recompensas inicial llamada `P`. Podemos pensar en ella como una matriz que tiene el número de estados como filas y el número de acciones como columnas.\n",
    "\n",
    "Dado que cada estado está en esta matriz, podemos ver los valores de recompensa predeterminados asignados al estado de nuestra ilustración:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 324\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 1, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.unwrapped.s = state\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 424, -1, False)],\n",
       " 1: [(1.0, 224, -1, False)],\n",
       " 2: [(1.0, 344, -1, False)],\n",
       " 3: [(1.0, 324, -1, False)],\n",
       " 4: [(1.0, 324, -10, False)],\n",
       " 5: [(1.0, 324, -10, False)]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[324]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este diccionario tiene la estructura {acción: [(probabilidad, próximo estado, recompensa, hecho)]}.\n",
    "\n",
    "Algunas cosas a tener en cuenta:\n",
    "\n",
    "* Los números del 0 al 5 corresponden a las acciones (sur, norte, este, oeste, recoger, dejar) que el taxi puede realizar en nuestro estado actual en la ilustración.\n",
    "* En este entorno, la probabilidad siempre es 1.0.\n",
    "* El próximo estado es el estado en el que estaríamos si tomamos la acción en este índice del diccionario.\n",
    "* Todas las acciones de movimiento tienen una recompensa de -1 y las acciones de recoger/dejar tienen una recompensa de -10 en este estado en particular. Si estamos en un estado donde el taxi tiene un pasajero y está encima del destino correcto, veríamos una recompensa de 20 en la acción de dejar (5).\n",
    "* done se utiliza para indicarnos cuándo hemos dejado con éxito a un pasajero en la ubicación correcta. Cada entrega exitosa es el final de un episodio.\n",
    "\n",
    "Ten en cuenta que si nuestro agente elige explorar la acción dos (2) en este estado, estaría yendo hacia el Este, hacia una pared. El código fuente ha hecho imposible mover realmente el taxi a través de una pared, así que si el taxi elige esa acción, simplemente seguirá acumulando penalizaciones de -1, lo que afecta a la recompensa a largo plazo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 427\n",
      "Penalties incurred: 151\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n",
    "env.reset()\n",
    "env.unwrapped.s = 324  # set environment to illustration's state\n",
    "\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, trunc, info = env.step(action)\n",
    "    # Añade +1 a la variable penaltie cuando el reward sea -10\n",
    "    if reward == -10:\n",
    "        penalties = penalties + 1\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[43m \\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 324,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[43m \\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 344,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[43m \\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 344,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 444,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 444,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[43m \\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 424,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[43m \\x1b[0m: |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 424,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 444,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 444,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 444,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 444,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[43m \\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 444,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[43m \\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 344,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[43m \\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 344,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 244,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : |\\x1b[43m \\x1b[0m: : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 144,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : |\\x1b[43m \\x1b[0m: : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 144,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : |\\x1b[43m \\x1b[0m: : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 144,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : |\\x1b[43m \\x1b[0m: : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 144,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 164,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 264,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 244,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 244,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 244,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 244,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 244,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 244,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 244,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : :\\x1b[43m \\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 244,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 264,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 264,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 164,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 264,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 284,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 264,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 364,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 264,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 264,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 284,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 184,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 184,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 284,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 284,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 184,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 184,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 164,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 64,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 64,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 164,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 164,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 164,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 164,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 184,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 84,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 184,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 164,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 164,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : |\\x1b[43m \\x1b[0m: : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 144,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 44,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 64,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 64,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 64,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 84,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 84,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 96,\n",
       "  'action': 4,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 84,\n",
       "  'action': 5,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 184,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 164,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 264,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 164,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 164,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 164,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 164,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 164,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 84,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 84,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 64,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 84,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 184,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 284,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 384,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 384,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 284,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 184,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 84,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 184,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 164,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 64,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 64,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 64,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 64,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 84,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 84,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 64,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 64,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 164,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 164,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 164,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 164,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 64,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 64,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 44,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: |\\x1b[43m \\x1b[0m: :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 44,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 64,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 84,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 96,\n",
       "  'action': 4,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 96,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 96,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 84,\n",
       "  'action': 5,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 84,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 184,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 284,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 284,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 284,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 284,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 384,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 384,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 284,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 384,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 384,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 284,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 284,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 284,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 184,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 284,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 284,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 384,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 384,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 364,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 264,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 264,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 364,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 364,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 384,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 364,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 364,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 364,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 364,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 384,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 384,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 384,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 384,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 384,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 364,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 364,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 364,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 264,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 364,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 364,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 364,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 364,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 264,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 284,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 384,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 384,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 464,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 484,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 384,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 484,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 464,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (East)\\n',\n",
       "  'state': 484,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B:\\x1b[43m \\x1b[0m|\\n+---------+\\n  (South)\\n',\n",
       "  'state': 484,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 384,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 384,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 284,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 284,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 264,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 264,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | :\\x1b[43m \\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 164,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 284,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 264,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 364,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 364,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 464,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[43mB\\x1b[0m: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 464,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 364,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 364,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | :\\x1b[43m \\x1b[0m|\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 384,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : |\\x1b[43m \\x1b[0m: |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 364,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : :\\x1b[43m \\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 264,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : : |\\n| : : : :\\x1b[43m \\x1b[0m|\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 284,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 184,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 184,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 184,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 184,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 84,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 96,\n",
       "  'action': 4,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 84,\n",
       "  'action': 5,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1m\\x1b[43mG\\x1b[0m\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 84,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 96,\n",
       "  'action': 4,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 96,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 96,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 96,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[42mG\\x1b[0m|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 96,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : :\\x1b[42m_\\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 196,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : :\\x1b[42m_\\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 196,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | :\\x1b[42m_\\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 176,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | :\\x1b[42m_\\x1b[0m: |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 176,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : :\\x1b[42m_\\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 276,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : :\\x1b[42m_\\x1b[0m: |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 276,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : :\\x1b[42m_\\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 256,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : :\\x1b[42m_\\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 256,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : :\\x1b[42m_\\x1b[0m: : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 256,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[42m_\\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 356,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 456,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 456,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 456,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 436,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 436,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 456,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[42m_\\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 356,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 336,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[42m_\\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 356,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[42m_\\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 356,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[42m_\\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 356,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 336,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 236,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 216,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 216,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n|\\x1b[42m_\\x1b[0m| : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 316,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n|\\x1b[42m_\\x1b[0m| : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 316,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|\\x1b[42mY\\x1b[0m| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 416,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n|\\x1b[42m_\\x1b[0m| : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 316,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 216,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 216,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 116,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 116,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 16,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 16,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 16,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 16,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 116,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 16,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 116,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 216,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 116,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 116,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 116,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 116,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| :\\x1b[42m_\\x1b[0m| : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 136,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 236,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 336,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 336,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 456,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 456,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 456,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 456,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 456,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 336,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 436,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 436,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 336,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 336,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 436,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 436,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 456,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 456,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 456,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 456,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 456,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | :\\x1b[42m_\\x1b[0m| : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 356,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| :\\x1b[42m_\\x1b[0m|B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 456,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y|\\x1b[42m_\\x1b[0m: |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 436,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| |\\x1b[42m_\\x1b[0m: | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 336,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 236,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 236,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 236,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 236,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 216,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n|\\x1b[42m_\\x1b[0m| : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 316,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n|\\x1b[42m_\\x1b[0m| : | : |\\n|Y| : |B: |\\n+---------+\\n  (Pickup)\\n',\n",
       "  'state': 316,\n",
       "  'action': 4,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 216,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 216,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 116,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 116,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 216,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n|\\x1b[42m_\\x1b[0m: : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 216,\n",
       "  'action': 5,\n",
       "  'reward': -10},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 236,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| :\\x1b[42m_\\x1b[0m| : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 136,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| : | : : |\\n| :\\x1b[42m_\\x1b[0m: : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 236,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| :\\x1b[42m_\\x1b[0m| : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 136,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m:\\x1b[42m_\\x1b[0m| : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 36,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m:\\x1b[42m_\\x1b[0m| : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 36,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m:\\x1b[42m_\\x1b[0m| : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 36,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m:\\x1b[42m_\\x1b[0m| : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 36,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 16,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (South)\\n',\n",
       "  'state': 116,\n",
       "  'action': 0,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n|\\x1b[42m_\\x1b[0m: | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 116,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m: | : :G|\\n| :\\x1b[42m_\\x1b[0m| : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 136,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m:\\x1b[42m_\\x1b[0m| : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (North)\\n',\n",
       "  'state': 36,\n",
       "  'action': 1,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35mR\\x1b[0m:\\x1b[42m_\\x1b[0m| : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (East)\\n',\n",
       "  'state': 36,\n",
       "  'action': 2,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[42mR\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (West)\\n',\n",
       "  'state': 16,\n",
       "  'action': 3,\n",
       "  'reward': -1},\n",
       " {'frame': '+---------+\\n|\\x1b[35m\\x1b[34;1m\\x1b[43mR\\x1b[0m\\x1b[0m\\x1b[0m: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n  (Dropoff)\\n',\n",
       "  'state': 0,\n",
       "  'action': 5,\n",
       "  'reward': 20}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        time.sleep(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.2)\n",
    "        \n",
    "# print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar un algoritmo simple de aprendizaje por refuerzo llamado Q-learning, que le dará a nuestro agente algo de memoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente, el Q-learning permite que el agente utilice las recompensas del entorno para aprender, con el tiempo, la mejor acción a tomar en un estado dado.\n",
    "\n",
    "En nuestro entorno Taxi, tenemos la tabla de recompensas, P, de la que el agente aprenderá. Lo hace al recibir una recompensa por tomar una acción en el estado actual, y luego actualiza un valor Q para recordar si esa acción fue beneficiosa.\n",
    "\n",
    "Los valores almacenados en la tabla Q se llaman valores Q, y se corresponden con una combinación (estado, acción).\n",
    "\n",
    "Un valor Q para una combinación particular de estado-acción es representativo de la \"calidad\" de una acción tomada desde ese estado. Mejores valores Q implican mejores posibilidades de obtener recompensas mayores.\n",
    "\n",
    "Por ejemplo, si el taxi se enfrenta a un estado que incluye un pasajero en su ubicación actual, es muy probable que el valor Q para recoger sea más alto en comparación con otras acciones, como dejar o ir hacia el norte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos asignando (), o actualizando, el valor Q del estado y la acción actual del agente primero tomando un peso () del antiguo valor Q, y luego añadiendo el valor aprendido. El valor aprendido es una combinación de la recompensa por tomar la acción actual en el estado actual, y la recompensa máxima descontada del próximo estado en el que estaremos una vez que tomemos la acción actual.\n",
    "\n",
    "Básicamente, estamos aprendiendo la acción adecuada a tomar en el estado actual al observar la recompensa para la combinación estado/acción actual y las máximas recompensas para el próximo estado. Esto eventualmente hará que nuestro taxi considere la ruta con las mejores recompensas concatenadas.\n",
    "\n",
    "El valor Q de un par estado-acción es la suma de la recompensa instantánea y la recompensa futura descontada (del estado resultante). La forma en que almacenamos los valores Q para cada estado y acción es a través de una tabla Q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla Q es una matriz donde tenemos una fila para cada estado (500) y una columna para cada acción (6). Se inicializa primero en 0, y luego los valores se actualizan después del entrenamiento. Ten en cuenta que la tabla Q tiene las mismas dimensiones que la tabla de recompensas, pero tiene un propósito completamente diferente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/q-matrix-initialized-to-learned_gQq0BFs.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosándolo en pasos, obtenemos:\n",
    "\n",
    "* Inicializar la tabla Q con todos los valores en cero.\n",
    "* Comenzar a explorar acciones: Para cada estado, seleccionar una de entre todas las acciones posibles para el estado actual (S).\n",
    "* Viajar al siguiente estado (S') como resultado de esa acción (a).\n",
    "* Para todas las acciones posibles desde el estado (S'), seleccionar aquella con el valor Q más alto.\n",
    "* Actualizar los valores de la tabla Q usando la ecuación.\n",
    "* Establecer el siguiente estado como el estado actual.\n",
    "* Si se alcanza el estado objetivo, entonces finalizar y repetir el proceso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de suficiente exploración aleatoria de acciones, los valores Q tienden a converger, sirviendo a nuestro agente como una función de valor de acción que puede explotar para elegir la acción más óptima a partir de un estado dado.\n",
    "\n",
    "Existe un compromiso entre la exploración (elegir una acción al azar) y la explotación (elegir acciones basadas en los valores Q ya aprendidos). Queremos evitar que la acción siempre tome la misma ruta, y posiblemente sobreajuste, así que introduciremos otro parámetro llamado \"epsilon\" para atender a esto durante el entrenamiento.\n",
    "\n",
    "En lugar de simplemente seleccionar la mejor acción aprendida con valor Q, a veces favoreceremos la exploración adicional del espacio de acciones. Un valor epsilon más bajo resulta en episodios con más penalizaciones (en promedio), lo que es obvio porque estamos explorando y tomando decisiones al azar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a construirlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Crea un array2d de ceros del tamaño de los diferentes estados y las diferentes posibles acciones\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear el algoritmo de entrenamiento que actualizará esta tabla Q a medida que el agente explore el entorno a lo largo de miles de episodios.\n",
    "\n",
    "En la primera parte de while not done, decidimos si elegir una acción al azar o explotar los valores Q ya calculados. Esto se hace simplemente utilizando el valor de epsilon y comparándolo con la función random.uniform(0, 1), que devuelve un número arbitrario entre 0 y 1.\n",
    "\n",
    "Ejecutamos la acción elegida en el entorno para obtener el próximo estado y la recompensa por realizar la acción. Después de eso, calculamos el valor Q máximo para las acciones correspondientes al próximo estado, y con eso, podemos actualizar fácilmente nuestro valor Q al nuevo valor_q:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# \"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, trunc, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que la tabla Q se ha establecido durante 100,000 episodios, veamos cuáles son los valores Q en el estado de nuestra ilustración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 424, -1, False)],\n",
       " 1: [(1.0, 224, -1, False)],\n",
       " 2: [(1.0, 344, -1, False)],\n",
       " 3: [(1.0, 324, -1, False)],\n",
       " 4: [(1.0, 324, -10, False)],\n",
       " 5: [(1.0, 324, -10, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.48883451, -2.4887257 , -2.48868632, -2.48891363, -6.83967822,\n",
       "       -6.37329423])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accede a los valores de la q-table del estado 324\n",
    "q_table[324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(q_table[324])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor máximo de Q es \"north\" o \"east\" (-2.489), ¡así que parece que Q-learning ha aprendido efectivamente la mejor acción a tomar en el estado de nuestra ilustración!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a evaluar el rendimiento de nuestro agente. Ya no necesitamos explorar acciones más, así que ahora la siguiente acción siempre se selecciona usando el mejor valor Q:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Ejecuta la accion\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m state, reward, done, trunc, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m: env\u001b[38;5;241m.\u001b[39mrender(),\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m }\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Actualiza el valor de penalties si el reward es -10\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:256\u001b[0m, in \u001b[0;36mTaxiEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[0;32m    255\u001b[0m     transitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms][a]\n\u001b[1;32m--> 256\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[43mcategorical_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_random\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     p, s, r, t \u001b[38;5;241m=\u001b[39m transitions[i]\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms \u001b[38;5;241m=\u001b[39m s\n",
      "File \u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\toy_text\\utils.py:8\u001b[0m, in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m      6\u001b[0m prob_n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(prob_n)\n\u001b[0;32m      7\u001b[0m csprob_n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(prob_n)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsprob_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp_random\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 10\n",
    "frames = []\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    # Crea el estado inicial\n",
    "    # state = env.encode(3, 1, 2, 0)\n",
    "    # env.reset()\n",
    "    # env.s = state\n",
    "    # Inicializa las epochs, penalties y rewards\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "    actions = []\n",
    "    while not done:\n",
    "        \n",
    "        # Elige la accion que te indique el maximo valor de la q_table\n",
    "        action = np.argmax(q_table[state])\n",
    "        actions.append(action)\n",
    "        # Ejecuta la accion\n",
    "        state, reward, done, trunc, info = env.step(action)\n",
    "        \n",
    "        frames.append({\n",
    "        'frame': env.render(),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "        )\n",
    "        # Actualiza el valor de penalties si el reward es -10\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        \n",
    "    # frames = []\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 129\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 3, 1, 4, 0, 0, 0, 0, 5]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en la evaluación que el rendimiento del agente mejoró significativamente y no incurrió en penalizaciones, lo que significa que realizó las acciones correctas de recogida/dejar con 100 pasajeros diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 2737.22\n",
      "Average penalties per episode: 884.9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance without Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    # Crea el estado inicial\n",
    "    state = env.encode(3, 1, 2, 0)\n",
    "    env.s = state\n",
    "    # Inicializa las epochs, penalties y rewards\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "    actions = []\n",
    "    while not done:\n",
    "        # Elige la acción random\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        # Ejecuta la accion\n",
    "        state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Actualiza el valor de penalties si el reward es -10\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA (State-Action-Reward-State-Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA es un algoritmo específico de aprendizaje por refuerzo que se utiliza para actualizar los valores de acción en función de la observación del siguiente estado y acción, además de la recompensa actual. En SARSA, se elige una acción (A) en un estado (S), se observa el siguiente estado (S') y se elige una nueva acción (A') basada en una política de toma de decisiones (que puede ser ε-greedy, por ejemplo). Luego, se actualizan los valores de acción utilizando la recompensa recibida y el valor de acción del siguiente estado y acción.\n",
    "\n",
    "\n",
    "\n",
    "La principal diferencia entre Q-Table y SARSA radica en cómo se actualizan los valores de acción.\n",
    "En la Q-Table, los valores se actualizan considerando el máximo valor de acción posible en el siguiente estado, independientemente de la acción tomada. En cambio, SARSA actualiza los valores de acción utilizando la acción real tomada en el siguiente estado.\n",
    "Por lo tanto, mientras que Q-Table es un método off-policy (actualiza los valores de acción considerando la mejor acción posible), SARSA es un método on-policy (actualiza los valores de acción considerando la acción real tomada).\n",
    "\n",
    "\n",
    "Q-Table:\n",
    "- Ventajas: Es simple de entender e implementar en entornos con un número limitado de estados y acciones.\n",
    "- Inconvenientes: No es escalable para entornos con un gran número de estados y acciones debido a la necesidad de almacenar y actualizar una tabla grande de valores de acción.\n",
    "\n",
    "SARSA:\n",
    "- Ventajas: Es más eficiente en términos de memoria y puede escalar mejor a entornos con un gran número de estados y acciones.\n",
    "- Inconvenientes: Puede ser más difícil de implementar y entender en comparación con Q-Table debido a la necesidad de seguir una política de toma de decisiones y actualizar los valores de acción de manera adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicialización de la tabla Q\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hiperparámetros\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# Para el registro de métricas\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()[0]\n",
    "    action = env.action_space.sample() if random.uniform(0, 1) < epsilon else np.argmax(q_table[state])\n",
    "\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done, trunc, info = env.step(action) \n",
    "        next_action = env.action_space.sample() if random.uniform(0, 1) < epsilon else np.argmax(q_table[next_state])\n",
    "\n",
    "        old_value = q_table[state, action]\n",
    "        next_value = q_table[next_state, next_action]\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_value)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        epochs += 1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q-Learning\n",
    "# action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "# # SARSA\n",
    "# next_action = np.argmax(q_table[next_state]) # Choose next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha (α):\n",
    "El parámetro alpha controla la tasa de aprendizaje en los algoritmos de aprendizaje por refuerzo. Es una medida de cuánto confiamos en las nuevas actualizaciones de los valores de acción en comparación con los valores existentes.\n",
    "\n",
    "- Si alpha es alto, damos más peso a las nuevas recompensas para actualizar los valores de acción.\n",
    "- Si alpha es bajo, damos más peso a los valores de acción existentes y aprendemos más lentamente.\n",
    "\n",
    "Ejemplo: Imagina que estás aprendiendo a jugar al ajedrez. Si tienes un alpha alto, estarías cambiando tus estrategias rápidamente después de cada partida. Si tienes un alpha bajo, estarías más inclinado a mantener tus estrategias existentes durante más tiempo, incluso si no te están dando buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma (γ):\n",
    "El parámetro gamma es el factor de descuento en los algoritmos de aprendizaje por refuerzo. Controla cuánto valoramos las recompensas futuras en comparación con las recompensas inmediatas.\n",
    "- Un gamma cercano a 1 significa que valoramos mucho las recompensas futuras.\n",
    "- Un gamma cercano a 0 significa que solo valoramos las recompensas inmediatas.\n",
    "\n",
    "Ejemplo: Imagina que estás decidiendo si estudiar para un examen o salir con tus amigos. Si tienes un gamma alto, estarías más inclinado a estudiar porque valoras mucho las recompensas futuras (buenas calificaciones). Si tienes un gamma bajo, estarías más inclinado a salir con tus amigos porque solo valoras la recompensa inmediata (diversión)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon (ε):\n",
    "El parámetro epsilon controla la exploración frente a la explotación en los algoritmos de aprendizaje por refuerzo. Determina la probabilidad de elegir una acción al azar en lugar de la acción óptima según los valores de acción actuales.\n",
    "- Un epsilon alto significa que somos más propensos a explorar nuevas acciones.\n",
    "- Un epsilon bajo significa que somos más propensos a explotar las acciones conocidas.\n",
    "\n",
    "Ejemplo: Imagina que estás decidiendo qué película ver en Netflix. Si tienes un epsilon alto, es más probable que explores nuevas películas en lugar de ver tus favoritas. Si tienes un epsilon bajo, es más probable que veas tus películas favoritas una y otra vez sin explorar nuevas opciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
