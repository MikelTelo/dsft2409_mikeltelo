{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_eda_libraries():\n",
    "    \"\"\"\n",
    "    Installs the most common libraries for performing Exploratory Data Analysis (EDA),\n",
    "    excluding less frequently used ones.\n",
    "    \"\"\"\n",
    "    libraries = [\n",
    "        # Data manipulation and processing\n",
    "        'pandas',       # Tabular data manipulation\n",
    "        'numpy',        # Numerical operations\n",
    "        'scipy',        # Mathematical and statistical calculations\n",
    "\n",
    "        # Visualization\n",
    "        'matplotlib',   # Basic plots\n",
    "        'seaborn',      # Advanced visualization\n",
    "        'plotly',       # Interactive plots\n",
    "        'missingno',    # Visualization of missing data\n",
    "\n",
    "        # Statistical Analysis\n",
    "        'statsmodels',  # Statistical models\n",
    "\n",
    "        # Data Profiling\n",
    "        'pandas-profiling',  # Automatic EDA reports (now known as ydata-profiling)\n",
    "\n",
    "        # Encoding and manipulating categorical variables\n",
    "        'category_encoders',  # Tools for encoding categorical variables\n",
    "\n",
    "        # Correlation Analysis\n",
    "        'phik',  # Advanced correlation metrics\n",
    "\n",
    "        # Multivariate Analysis\n",
    "        'yellowbrick',  # Visualization of ML metrics and data analysis\n",
    "\n",
    "        # Missing Data Management\n",
    "        'sklearn-pandas'  # Preprocessing pipelines with sklearn integration\n",
    "    ]\n",
    "    \n",
    "    for library in libraries:\n",
    "        try:\n",
    "            __import__(library)\n",
    "            print(f\"‚úÖ The library '{library}' is already installed.\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ö†Ô∏è Installing '{library}'...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
    "    print(\"\\nüöÄ All necessary libraries for EDA are installed and ready to use.\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    install_eda_libraries()\n",
    "\n",
    "\n",
    "#python install_eda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_eda_libraries():\n",
    "    \"\"\"\n",
    "    Imports the most common libraries for Exploratory Data Analysis (EDA)\n",
    "    and configures them for optimal use.\n",
    "    \"\"\"\n",
    "    global pd, np, plt, sns, missingno, statsmodels, sp, go, profile, ce, phik, yellowbrick\n",
    "    \n",
    "    try:\n",
    "        # üìä Data Manipulation and Processing\n",
    "        import pandas as pd       # Tabular data manipulation\n",
    "        import numpy as np        # Numerical operations\n",
    "        import scipy as sp        # Mathematical and statistical calculations\n",
    "        \n",
    "        # üìà Visualization\n",
    "        import matplotlib.pyplot as plt  # Basic plots\n",
    "        import seaborn as sns     # Advanced statistical plots\n",
    "        import plotly.graph_objects as go  # Interactive plots\n",
    "        import missingno          # Visualization of missing data patterns\n",
    "        \n",
    "        # üìä Statistical Analysis\n",
    "        import statsmodels.api as sm  # Statistical models\n",
    "        \n",
    "        # üìë Data Profiling\n",
    "        from ydata_profiling import ProfileReport as profile  # Automatic data profiling reports\n",
    "        \n",
    "        # üè∑Ô∏è Categorical Variable Encoding\n",
    "        import category_encoders as ce  # Advanced encoding methods for categorical variables\n",
    "        \n",
    "        # üîó Correlation Analysis\n",
    "        import phik  # Correlation analysis between categorical and numerical variables\n",
    "        \n",
    "        # üìä Multivariate Analysis\n",
    "        import yellowbrick  # Visualization of ML metrics and data patterns\n",
    "        \n",
    "        # ‚öôÔ∏è Basic Configurations\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.style.use('ggplot')\n",
    "        pd.options.display.float_format = '{:.2f}'.format  # Display floats with two decimals\n",
    "        \n",
    "        print(\"‚úÖ All libraries for EDA have been successfully imported.\")\n",
    "    \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Error importing libraries: {e}\")\n",
    "        print(\"üîÑ Make sure you have executed the library installation function first.\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    import_eda_libraries()\n",
    "\n",
    "\n",
    "#python import_eda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_ml_libraries():\n",
    "    \"\"\"\n",
    "    Installs the most common libraries for Machine Learning, excluding less frequently used ones.\n",
    "    \"\"\"\n",
    "    libraries = [\n",
    "        # üìä Data Manipulation and Processing\n",
    "        'numpy',              # Numerical operations\n",
    "        'pandas',             # Tabular data manipulation\n",
    "        'scipy',              # Mathematical and statistical calculations\n",
    "        \n",
    "        # üìà Visualization\n",
    "        'matplotlib',         # Static plots\n",
    "        'seaborn',            # Advanced visualization\n",
    "        'plotly',             # Interactive plots\n",
    "        \n",
    "        # ü§ñ General Machine Learning\n",
    "        'scikit-learn',       # Classic Machine Learning algorithms\n",
    "        'xgboost',            # Gradient Boosting for tabular data\n",
    "        'lightgbm',           # Efficient Gradient Boosting\n",
    "        'catboost',           # Optimized Gradient Boosting\n",
    "        \n",
    "        # üß† Deep Learning\n",
    "        'tensorflow',         # Neural networks and Deep Learning\n",
    "        'keras',              # High-level API for TensorFlow\n",
    "        'torch',              # Deep Learning framework (PyTorch)\n",
    "        'transformers',       # Advanced NLP models (BERT, GPT, etc.)\n",
    "        \n",
    "        # üîç Model Optimization\n",
    "        'optuna',             # Hyperparameter optimization\n",
    "        'hyperopt',           # Alternative for hyperparameter optimization\n",
    "        \n",
    "        # üì¶ Model Tracking and Management\n",
    "        'mlflow',             # ML experiment tracking\n",
    "        'dvc',               # Data version control\n",
    "        \n",
    "        # üìä Statistical Analysis\n",
    "        'statsmodels',        # Statistical models\n",
    "        \n",
    "        # ‚öôÔ∏è Text and NLP Processing\n",
    "        'nltk',              # Natural Language Processing\n",
    "        'spacy',             # Efficient NLP processing\n",
    "        \n",
    "        # üèóÔ∏è Data Processing\n",
    "        'imblearn',           # Handling imbalanced data\n",
    "        'joblib',            # Model serialization\n",
    "        \n",
    "        # üìä Visualization and Analysis\n",
    "        'yellowbrick',        # ML metrics visualization\n",
    "        \n",
    "        # üõ°Ô∏è Model Validation\n",
    "        'shap',              # Model interpretability\n",
    "        'lime',              # Local interpretation of predictions\n",
    "        \n",
    "        # üîó Neural Network Graphs\n",
    "        'networkx',           # Graph modeling and analysis\n",
    "        \n",
    "        # ‚ö° Parallel Computing\n",
    "        'dask',              # Parallel data processing\n",
    "        'ray',               # Distributed ML tasks and models\n",
    "        \n",
    "        # üìë Feature Engineering\n",
    "        'feature-engine',    # Advanced feature transformations\n",
    "    ]\n",
    "    \n",
    "    for library in libraries:\n",
    "        try:\n",
    "            __import__(library)\n",
    "            print(f\"‚úÖ The library '{library}' is already installed.\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ö†Ô∏è Installing '{library}'...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
    "    \n",
    "    print(\"\\nüöÄ All necessary libraries for Machine Learning are installed and ready to use.\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    install_ml_libraries()\n",
    "\n",
    "#python install_ml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_ml_libraries():\n",
    "    \"\"\"\n",
    "    Imports the most common libraries for Machine Learning and sets up basic configurations.\n",
    "    \"\"\"\n",
    "    global pd, np, plt, sns, go\n",
    "    global tf, keras, xgb, lgb, cb, sm, sp, torch, transformers\n",
    "    global train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "    global accuracy_score, confusion_matrix, classification_report\n",
    "    global optuna, shap, lime, dask, ray, imblearn\n",
    "    global feature_engine\n",
    "    \n",
    "    try:\n",
    "        # üìä Data Manipulation\n",
    "        import pandas as pd       # Tabular data manipulation\n",
    "        import numpy as np        # Numerical operations\n",
    "        import scipy as sp        # Mathematical and statistical calculations\n",
    "        \n",
    "        # üìà Visualization\n",
    "        import matplotlib.pyplot as plt  # Basic plots\n",
    "        import seaborn as sns     # Advanced statistical plots\n",
    "        import plotly.graph_objects as go  # Interactive plots\n",
    "        \n",
    "        # ü§ñ Machine Learning\n",
    "        import sklearn\n",
    "        from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "        import xgboost as xgb     # Gradient Boosting\n",
    "        import lightgbm as lgb    # Efficient Gradient Boosting\n",
    "        import catboost as cb     # Optimized Gradient Boosting\n",
    "        \n",
    "        # üß† Deep Learning\n",
    "        import tensorflow as tf   # Neural networks\n",
    "        from tensorflow import keras\n",
    "        import torch              # Deep Learning framework\n",
    "        from transformers import pipeline  # NLP models (GPT, BERT)\n",
    "        \n",
    "        # üîç Model Optimization\n",
    "        import optuna             # Hyperparameter optimization\n",
    "        \n",
    "        # üõ°Ô∏è Model Interpretability\n",
    "        import shap              # Global interpretability\n",
    "        import lime              # Local interpretability\n",
    "        \n",
    "        # ‚ö° Parallel Computing\n",
    "        import dask              # Parallel data processing\n",
    "        import ray               # Distributed ML models and tasks\n",
    "        \n",
    "        # üèóÔ∏è Data Processing\n",
    "        import imblearn          # Handling imbalanced data (SMOTE, ADASYN)\n",
    "        import feature_engine    # Advanced feature transformations\n",
    "        \n",
    "        # üìä Statistical Analysis\n",
    "        import statsmodels.api as sm  # Statistical models\n",
    "        \n",
    "        # ‚öôÔ∏è Basic Configurations\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.style.use('ggplot')\n",
    "        pd.options.display.float_format = '{:.2f}'.format  # Display floats with two decimal places\n",
    "        \n",
    "        print(\"‚úÖ All libraries for Machine Learning have been successfully imported.\")\n",
    "    \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Error importing libraries: {e}\")\n",
    "        print(\"üîÑ Make sure you have executed the library installation function first.\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    import_ml_libraries()\n",
    "\n",
    "#python import_ml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical_variables(df, method='auto', one_hot_threshold=10, preferences=None):\n",
    "    \"\"\"\n",
    "    Converts categorical variables to numerical using One-Hot Encoding or Label Encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing categorical variables.\n",
    "    - method (str): 'auto' for automatic selection, 'onehot' for One-Hot Encoding, 'label' for Label Encoding.\n",
    "    - one_hot_threshold (int): Maximum number of categories for applying One-Hot Encoding in 'auto' mode.\n",
    "    - preferences (dict): Dictionary with specific columns and their encoding method.\n",
    "      Example: {'column1': 'onehot', 'column2': 'label'}\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with transformed categorical variables.\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    for column in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        unique_count = df[column].nunique()\n",
    "        \n",
    "        # If user specifies a method for the column\n",
    "        if preferences and column in preferences:\n",
    "            column_method = preferences[column]\n",
    "        else:\n",
    "            # If no specific preference, use the general method\n",
    "            if method == 'auto':\n",
    "                column_method = 'onehot' if unique_count <= one_hot_threshold else 'label'\n",
    "            else:\n",
    "                column_method = method\n",
    "        \n",
    "        # Apply the selected method\n",
    "        if column_method == 'onehot':\n",
    "            print(f\"üîπ Applying One-Hot Encoding to '{column}' (Categories: {unique_count})\")\n",
    "            dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "            transformed_df = pd.concat([transformed_df.drop(column, axis=1), dummies], axis=1)\n",
    "        \n",
    "        elif column_method == 'label':\n",
    "            print(f\"üî∏ Applying Label Encoding to '{column}' (Categories: {unique_count})\")\n",
    "            transformed_df[column] = label_encoder.fit_transform(df[column])\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unrecognized method for '{column}': {column_method}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Transformation completed.\")\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red'],\n",
    "    'Size': ['Large', 'Small', 'Medium', 'Large'],\n",
    "    'Category': ['A', 'B', 'C', 'D']\n",
    "})\n",
    "\n",
    "# Automatic mode\n",
    "print(\"üîÑ Automatic Mode:\")\n",
    "df_auto = encode_categorical_variables(df, method='auto', one_hot_threshold=2)\n",
    "\n",
    "# Manual preferences mode\n",
    "print(\"\\nüîÑ Manual Preferences Mode:\")\n",
    "preferences = {'Color': 'onehot', 'Size': 'label'}\n",
    "df_manual = encode_categorical_variables(df, method='auto', preferences=preferences)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Automatic Mode Result:\")\n",
    "print(df_auto)\n",
    "\n",
    "print(\"\\nüìä Manual Preferences Mode Result:\")\n",
    "print(df_manual)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
